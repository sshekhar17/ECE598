\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\csxdef[2]{}
\@writefile{toc}{\providecommand\autonum@processReference[2]{}}
\@writefile{lof}{\providecommand\autonum@processReference[2]{}}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Information Measures}{1}{section.1}\protected@file@percent }
\newlabel{def:entropy-discrete}{{1.1}{1}{Entropy}{theorem.1.1}{}}
\newlabel{example:binary-entropy}{{1.2}{2}{}{theorem.1.2}{}}
\csxdef {autonum@fig:binary-entropy-klReferenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Binary entropy $h_b(p)$ on the left, and $D_{\text  {KL}}(p\parallel 1/2)$ and $D_{\text  {KL}}(1/2 \parallel p)$ on the right.}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:binary-entropy-kl}{{1}{2}{Binary entropy $h_b(p)$ on the left, and $\dkl (p\parallel 1/2)$ and $\dkl (1/2 \parallel p)$ on the right}{figure.caption.1}{}}
\newlabel{def:relative-entropy-discrete}{{1.3}{2}{Relative Entropy}{theorem.1.3}{}}
\newlabel{example:binary-kl}{{1.4}{3}{}{theorem.1.4}{}}
\csxdef {autonum@fig:binary-entropy-klReferenced}{}
\newlabel{def:mutual-information-discrete}{{1.5}{3}{Mutual Information}{theorem.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Extension to general alphabets}{3}{subsection.1.1}\protected@file@percent }
\newlabel{subsec:extention-to-general-alphabet}{{1.1}{3}{Extension to general alphabets}{subsection.1.1}{}}
\citation{renyi1959dimension}
\newlabel{def:relative-entropy-general}{{1.6}{4}{}{theorem.1.6}{}}
\newlabel{def:mutual-information-general}{{1.7}{4}{}{theorem.1.7}{}}
\newlabel{remark:general-entropy}{{1.8}{5}{}{theorem.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Properties of Information Measures}{5}{section.2}\protected@file@percent }
\newlabel{sec:properties-of-information-measures}{{2}{5}{Properties of Information Measures}{section.2}{}}
\newlabel{theorem:gibbs}{{2.1}{5}{Gibbs Inequality}{theorem.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Nonnegativity of Information Measures}{5}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Chain Rules}{6}{subsection.2.2}\protected@file@percent }
\newlabel{theorem:chain-rules}{{2.2}{6}{Chain Rules}{theorem.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Data Processing Inequalities}{6}{subsection.2.3}\protected@file@percent }
\newlabel{theorem:DPI}{{2.3}{7}{Data Processing Inequality}{theorem.2.3}{}}
\csxdef {autonum@theorem:DPIReferenced}{}
\newlabel{corollary:DPI}{{2.4}{7}{}{theorem.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Application: Extracting Purely Random Bits}{8}{section.3}\protected@file@percent }
\newlabel{eq:fair-coin-toss}{{1}{8}{Application: Extracting Purely Random Bits}{equation.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{The information theoretic limit.}{8}{section*.2}\protected@file@percent }
\csxdef {autonum@eq:fair-coin-tossReferenced}{}
\newlabel{eq:fundamental-limit}{{2}{8}{The information theoretic limit}{equation.3.2}{}}
\citation{peres1992iterating}
\citation{peres1992iterating}
\bibstyle{abbrvnat}
\bibdata{../ref}
\bibcite{peres1992iterating}{{1}{1992}{{Peres}}{{}}}
\bibcite{renyi1959dimension}{{2}{1959}{{R{\'e}nyi}}{{}}}
\@writefile{toc}{\contentsline {paragraph}{Von Neumann's (suboptimal) extractor.}{9}{section*.3}\protected@file@percent }
\newlabel{prop:von-neumann-extractor}{{3.1}{9}{}{theorem.3.1}{}}
\csxdef {autonum@eq:fair-coin-tossReferenced}{}
\newlabel{remark:peres}{{3.2}{9}{}{theorem.3.2}{}}
\csxdef {autonum@eq:fundamental-limitReferenced}{}
\gdef \@abspage@last{9}
