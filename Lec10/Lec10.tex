\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{../scribe}
\usepackage{listings}

\usepackage{hyperref}
% \usepackage{cleveref}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{tikz}
\renewcommand{\arraystretch}{1.5}
\usepackage{nicefrac}
% \newcommand{\thetahat}{\widehat{\theta}}
% \Scribe{Shubhanshu Shekhar}
\Lecturer{Shubhanshu Shekhar}
\LectureNumber{10}
\LectureDate{30th September, 2025}
\LectureTitle{Fano's Method for Minimax Lower Bounds}

\lstset{style=mystyle}
% \usepackage{tikz}
\usepackage{comment}
% \usepackage{pgfplots}
% \usepackage{enumitem}
\input{../preamble}
\input{../newMacros}


\begin{document}
	\MakeScribeTop
%#############################################################
%#############################################################
%#############################################################
%#############################################################

In this lecture, we begin our discussion of Fano's method for deriving minimax lower bounds. Like Assouad's method, this technique also proceeds via a reduction to $M$-ary hypothesis testing. However, unlike Assouad, it requires fewer structural assumptions~(no indexing by Hamming cube or separability of loss functions), and thus it is more generally applicable. 


\section{General Scheme} 
\label{sec:assouad-general}
For simplicity, we present the idea behind this technique in the context of minimax estimation. As always, we let $\calX$ denote some observation space, $\{P_\theta: \theta \in \Theta\}$ denotes a statistical model indexed by a psedometric space $(\Theta, \rho)$. For some non-decreasing function $\Phi:[0, \infty) \to [0, \infty)$, we define a loss function $L(\theta, \theta') = \Phi \circ \rho(\theta, \theta')$, and the risk associated with any estimator $\thetahat$ is defined as 
\begin{align}
R(\thetahat(X), \theta) = \mathbb{E}_{X \sim P_\theta}\lb L(\thetahat(X), \theta) \rb. 
\end{align}
The minimax risk in this scenario is equal to 
\begin{align}
R^*(\Theta, \Phi \circ \rho) = \inf_{\thetahat}\sup_{\theta \in \Theta}\mathbb{E}_{X \sim P_\theta}\lb L(\thetahat(X), \theta) \rb. 
\end{align}

\paragraph{Reduction to $M$-ary hypothesis testing.}  For some $M\geq 2$, let  $\Theta_M = \{\theta_i: i \in [M]\}$ denote a finite subset of the parameter space, and let $\{P_i \equiv P_{\theta_i}: i \in [M]\}$ denote the corresponding distributions. Furthermore, assume that the subset $\Theta_M$ is $2\delta$-separated in $\rho$; that is, $\rho(\theta_i, \theta_j) \geq 2\delta$ for all $ i \neq j \in [M]$. Then, we have the usual lower bound: 
\begin{align}
	R^* &\geq \inf_{\thetahat} \max_{i \in [M]} \mathbb{E}_{X \sim P_i} \lb L(\thetahat(X), \theta_i) \rb 
	 \geq \inf_{\thetahat} \; \mathbb{E}_{V}\lb \mathbb{E}_{X \sim P_V}\lb L(\thetahat(X), \theta_V) \rb \rb, 
\end{align}
where $V \sim P_V$ is any $[M]$-valued random variable. Introduce the term
\begin{align}
	w(\delta) = \inf_{\theta, \theta': \rho(\theta, \theta')\geq \delta} L(\theta, \theta'), 
\end{align}
and observe that 
\begin{align}
	L(\theta_V, \thetahat(X)) \geq w(\delta) \, \boldsymbol{1}\{ \rho(\theta_V, \thetahat(X)) \geq \delta \}. \label{eq:loss-lower-bound-fano-1}
\end{align}
Given any estimator $\thetahat$, we can turn it into a minimum distance estimator of $V$ as follows: 
\begin{align}
	\hat{\psi}(X) = \argmin_{i \in [M]} \rho(\theta_i, \thetahat(X)), 
\end{align}
breaking ties in a deterministic manner. This implies that we can rewrite~\eqref{eq:loss-lower-bound-fano-1}  as 
\begin{align}
	L(\theta_V, \thetahat(X)) \geq w(\delta) \, \boldsymbol{1}\{ \hat{\psi}(X) \neq V \}, 
\end{align}
which leads to 
\begin{align}
	R^* & \geq w(\delta) \, \inf_{\hat{V}:\calX \to [M]} \mathbb{P}_{X, V}\lp \hat{V}(X) \neq V \rp. \label{eq:fano-lower-bound-1}
\end{align}
In other words, we can lower bound the minimax estimation risk with $w(\delta)$ times the probability of error in the $M$-ary hypothesis testing problem. 


\paragraph{Fano's Inequality for $M$-ary Hypothesis Testing.} So we are interested in analyzing the following Markov chain 
\begin{align}
	V \longrightarrow X \longrightarrow \hat{V}. 
\end{align}
Let $p_e$ denote probability of error $\mathbb{P}(\hat{V} \neq V)$ associated with any estimator~(or $M$-ary hypothesis test) $\hat{V}$. Then, Fano's inequality tells us that 
\begin{align}
	p_e \log (M-1) + h_2(p_e) \geq H(V|\hat{V}) \geq H(V|X), 
\end{align}
where $h_2$ denotes the binary entropy. The second inequality above is due to the data processing inequality. Plugging this into~\eqref{eq:fano-lower-bound-1}, we get the general Fano's lower bound. 
\begin{theorem}
	\label{theorem:fano-main-1} Suppose $\{P_i \equiv P_{\theta_i}: i \in [M]\}$ denote a collection of distributions with associated parameters $\{\theta_i: i \in [M]\}$ satisfying $\rho(\theta_i, \theta_j) \geq 2\delta$ for all $i \neq j \in [M]$. Then, with $V$ denoting any $[M]$-valued random variable, and $X|(V=v) \sim P_v$, we have the following: 
	\begin{align}
		R^*(\Theta, L) \geq w(\delta) \lp \frac{H(V|X) - \log 2}{\log M} \rp \qtext{where} w(\delta) = \inf_{\theta, \theta': \rho(\theta, \theta') \geq \delta} L(\theta, \theta').  \label{eq:fano-lower-bound-2}
	\end{align}
	For the special case of $V \sim \uniform([M])$, then we can get the following lower bound: 
	\begin{align}
		\calR(\calP, L) \geq w(\delta)\, \lp 1 - \frac{I(V; X) + \log 2}{\log M} \rp. \label{eq:fano-lower-bound-3}
	\end{align}
\end{theorem}
\begin{proof}
The bound in~\eqref{eq:fano-lower-bound-2} follows from~\eqref{eq:fano-lower-bound-1} using the fact that $h_2(p_e) \leq \log 2$, $\log(M-1) \leq \log M$ and on rearranging. To obtain~\eqref{eq:fano-lower-bound-3}, simply use the fact that when $V \sim \uniform([M])$, then $H(V) = \log M$, and thus $I(V;X) = H(V) - H(V|X) = \log M - H(V|X)$. 
\end{proof}

Thus, in order to apply~Theorem~\ref{theorem:fano-main-1} to establish minimax rates for estimation problems, we need to do the following: 
\begin{itemize}
	\item[(a)] Identify a collection of $M$ distributions which are pairwise $2\delta$ separated
	\item[(b)] For some $[M]$-valued r.v. $V$, we need a lower bound on $H(V|X)$. Or with $V \sim \uniform([M])$, we need an upper bound on $I(V; X)$. 
\end{itemize}
We discuss some details of step~$(a)$ in~Section.~\ref{subsec:packing-sets}. For step $(b)$, a convenient approach is to use the relative entropy based definition of mutual information. In particular, we know that 
\begin{align}
	I(V; X) = \dkl(P_{XV} \parallel P_X \times P_V) = \dkl(P_{X|V} \parallel P_X | P_V)
\end{align}
When $V \sim \uniform([M])$, we have $P_X = (1/M) \sum_{i=1}^M P_i$, which implies that 
\begin{align}
	I(V; X) &= \dkl(P_{X|V} \parallel P_X \mid P_V) = \frac{1}{M} \sum_{i=1}^M \dkl(P_i \parallel P_X) \\
	& \leq \frac{1}{M^2} \sum_{i=1}^M \sum_{j=1}^M \dkl(P_i \parallel P_j) && (\text{convexity of } \dkl) \\
	& \leq \max_{i, j \in [M]} \dkl(P_i \parallel P_j). 
\end{align}
Thus, it suffices to construct a collection of distributions whose worst case relative entropy over all pairs can be controlled. We summarize this discussion in our next result. 

\begin{corollary}
	\label{corollary:fano-kl-version} Suppose $V \sim \uniform{[M]}$, and let $\{P_i: i \in [M]\}$ denote a collection of distributions in $\calP$ satisfying 
	\begin{align}
		\rho(\theta_i, \theta_j) \geq 2\delta, \quad \dkl(P_i \parallel P_j) \leq f(\delta), \qtext{and} \log M \geq 2(f(\delta) + \log 2),  \label{eq:fano-corollary-conditions}
	\end{align} 
	for some function $f:(0, \infty) \to (0, \infty)$. Then, we have 
	\begin{align}
		\calR(\calP, L) \geq \frac{w(\delta)}{2}. 
	\end{align}
\end{corollary}

\begin{remark}
	\label{remark:fano-corollary}
	\eqref{eq:fano-corollary-conditions} states the properties of the "hard subset of problems" that characterize the minimax rate of the estimation problem: we should be able to identify sufficiently many distributions~($\log M \geq 2(f(\delta) + \log 2)$) that are well separated in parameters space~($\rho(\theta_i, \theta_j)$), but are difficult to distinguish statistically~($\dkl(P_i\parallel P_j) \leq f(\delta)$). 
\end{remark}

\subsection{Construction of $2\delta$ separated distribution sets}
\label{subsec:packing-sets}
In most applications, in order to construct the collection of $2\delta$ separated disrtibutions $\calV = \{P_v: v \in [M]\}$, we will follow the following approach: 
\begin{itemize}
	\item We will construct a collection of distributions associated with an appropriate covering set of the parameter space $\Theta$~(or a small region of the parameter space). 
	\item Then, as needed, we will scale this set with a parameter $\delta$. 
\end{itemize}
The following two results will be useful in constructing the packing sets. 

\begin{lemma}
	\label{lemma:covering-euclidean} Let $B(0, 1) = \{\theta \in \mathbb{R}^d: \|\theta\|_2 \leq 1\}$ denote the $\ell_2$ unit ball  in $\mathbb{R}^d$. Then, there exists a set $\calV \subset B(0, 1)$ such that 
	\begin{itemize}
		\item For any $v \neq v' \in \calV$, we have $\|v - v'\|_2 \geq 1/2$. 
		\item The cardinality of $\calV$ satisfies $|\calV| \geq 2^d$. 
	\end{itemize}
\end{lemma}

\begin{proof}
	The proof of this statement follows by standard "volume arguments". In particular, suppose $\calV$ is a maximal set with the property that $\|v-v'\|_2 \geq 1/2$. Hence, for any $w \in B(0, 1)$ there must exist a $v \in \calV$ such that $\|v-w\|_2 < 1/2$. For if this were not true, then we could append this $w$ to $\calV$ while maintaining the separation condition, thus violating the ``maximal'' assumption on $\calV$. Another way of stating this condition is that 
	\begin{align}
		\cup_{v \in \calV} B(v, 1/2) \supset B(0, 1) \quad \implies \quad \mathrm{Vol}\lp \cup_{v \in \calV} B(v, 1/2)\rp \geq \mathrm{Vol}\lp B(0, 1) \rp, 
	\end{align}
	where $\mathrm{Vol}$ denote the usual volume~(i.e., Lebesgue measure) of the sets involved. Since the Lebesgue measure is translation invariant and subadditive, we know that $\mathrm{Vol}(B(v, 1/2)) = \mathrm{Vol}(B(0, 1/2))$ for all $v \in \calV$, and 
	\begin{align}
		|\calV|\, C_d (1/2)^d =  |\calV|\, \mathrm{Vol}(B(0,1/2)) \geq \mathrm{Vol}\lp \cup_{v \in \calV} B(v, 1/2)\rp \geq \mathrm{Vol}\lp B(0, 1) \rp = C_d 1^d,
	\end{align}
	for some $d$-dependent constant $C_d$. This leads to the required conclusion that $|\calV| \geq 2^d$. 
\end{proof}

Another important case is when we need to construct packing sets of the Hamming cube $\calH_d = \{-1, +1\}^d$. 
\begin{lemma}[Gilbert-Varshamov]
	\label{lemma:gilbert-varshamov} There exists a subset $\calV \subset \calH_d = \{-1, +1\}^d$ satisfying 
	\begin{itemize}
		\item For any $v \neq v' \in \calV$, we have $d_H(v, v') \geq d/4$.
		\item The cardinality of $\calV$ satisfies $|\calV| \geq e^{d/8}$. 
	\end{itemize}
	In other words there exists a subset of $\calH_d$ of  size growing exponentially in $d$ with pairwise separation at least $d/4$ in Hamming metric. 
\end{lemma}

\begin{proof}
Assume that $\calV$ is a maximal $d/4$-packing set in Hamming metric. Then, as before, we have 
\begin{align}
	\cup_{v \in \calV} B_H(v, d/4) \supset \calH_d, 
\end{align}	
where $B_H(v, d/4) = \{v' \in \calH_d: d_H(v, v') \leq d/4\}$. We can also show that $|B_H(v, d/4)|$ is independent of $v$, which means that for some fixed arbitrary element $v_0$ of $\calV$, we have $|\calV| |B_H(v_0, d/4)| \geq |\calH_d| = 2^d$. To conclude the proof, we will derive an appropriate upper bound on $|B_H(v_0, d/4)$. 

Let $S_1, \ldots, S_d$ denote \iid $\Bernoulli(1/2)$ random variables, using which we can define the random variable $V$ such that 
\begin{align}
	V[j] = v_0[j] \oplus S_j, \qtext{for all} j \in [d]. 
\end{align}
It is easy to verify that $V$ is uniformly distributed over the Hamming cube. Hence, we have 
\begin{align}
	\mathbb{P}\lp V \in B_H(v_0, d/4) \rp  = \frac{|B_H(v_0, d/4)|}{2^d} = \mathbb{P}\lp \sum_{j=1}^d S_j \leq d/4 \rp= \mathbb{P}\lp \sum_{j=1}^d S_j \geq 3d/4 \rp. 
\end{align}
The last inequality uses the fact that $S_j \stackrel{d}{=} 1-S_j$ for all $j \in [d]$. An application of Chernoff bound then implies  
\begin{align}
	 \frac{|B_H(v_0, d/4)|}{2^d}  \leq e^{-\lambda 3d/4} \mathbb{E}\lb e^{\lambda \sum_{i=1}^d S_i} \rb = e^{-\lambda 3d/4} \lp \frac{1 + e^\lambda}{2} \rp^d. 
\end{align}
It turns out that the upper bound is optimized at $\lambda=\log 3$, giving the bound 
\begin{align}
	|B_H(v_0, d/4)| \leq 4^d 3^{-3d/4}. 
\end{align}
Plugging this into the bound $|\calV| \geq 2^d / |B_H(v_0, d/4)|$, gives us 
\begin{align}
	|\calV| \geq 2^d 3^{3d/4} 4^{-d} = \exp \lp d \log \lp \frac{2 \times 3^{3/4}}{4} \rp\rp \geq e^{d/8}. 
\end{align}
This completes the proof. 
\end{proof}


\section{Nonparametric regression in $\sup$ norm}
\label{sec:fano-nonparametric-regression}


Let $\Theta \equiv \Theta(C, \gamma)$ denote the set of all H\"older smooth functions~(with  parameters $C \in (0, \infty)$ and $\gamma \in (0, 1]$)  supported on the unit interval $[0,1]$; that is, for every $x, x' \in [0,1]$, and $\theta \in \Theta$, we have 
\begin{align}
|\theta(x)-\theta(x')| \leq C |x-x'|^\gamma.
\end{align}
Suppose we have $n$ observations of the form 
\begin{align}
	Y_i = \theta(x_i) + \sigma \varepsilon_i, \qtext{where} x_i = i/n, \; (\varepsilon_i)_{i=1}^n \stackrel{\iid}{\sim} N(0, 1). 
\end{align}
For simplicity, we have assumed a ``fixed design''~(with $x_i = i/n$), but the same idea can be extended to the case of $X_i \sim P_X$ as well, for sufficiently regular $P_X$. 


The observation space in this problem is $\calX = \prod_{i=1}^n [0,1] \times \mathbb{R}$. The statistical model $\{P_\theta: \theta \in \Theta(C, \gamma)\}$ consists of distributions of the form 
\begin{align}
	P_\theta \equiv P_{\theta, X^n, Y^n} = \otimes_{i=1}^n \lp \delta_{x_i} \times N(\theta(x_i), \sigma^2) \rp. 
\end{align}
Thus, given the observations $Y^n$, our goal is to construct an estimate $\thetahat(Y^n)$ that achieves a small worst-case risk 
\begin{align}
R_n(\thetahat, \Theta) = \sup_{\theta \in \Theta} \mathbb{E}\lb \|\thetahat(Y^n) - \theta\|_2^2 \rb. 
\end{align}
This corresponds to $L(\theta, \theta') = \Phi \circ \rho(\theta, \theta')$ with $\rho$ denoting the metric induced by the $L^2$-norm (w.r.t. to the Lebesgue measure on $[0,1]$), and $\Phi$ denotes the mapping $t \mapsto t^2$.  

For simplicity, we will focus on the case of $\gamma=1$, which corresponds to Lipschitz functions. 
\subsection{Converse}
\label{subsec:nonparametric-regression-converse}
To establish a converse result using Fano's method, we need to construct an appropriate ``hard'' class of problems. To do this, we will proceed as follows: 
\begin{itemize}
	\item For some integer $m$ to be decided later, partition the unit interval into $m$ equal sub-intervals $I_i = [i-1/n, i/n)$ for $i \in [n-1]$, and $I_n = [1-1/n, 1]$. 
	\item Let us consider a bump function $\phi: [0, 1] \to [0,1]$ defined as 
	\begin{align}
		\phi(u) = \begin{cases}
		3u, & \text{ if } u \in [0, 1/3), \\
		1, & \text{if } u \in [1/3, 2/3), \\
		1-3u, & \text{if } u \in [2/3, 1]. 
		\end{cases}
	\end{align}
	We can verify that this function is Lipschitz with parameter $C=3$. 

	\item For any $j \geq 1$, define the scaled-and-shifted bump function $\phi_j$ as 
	\begin{align}
		\phi_j(x) = \phi\lp m\lp x - \frac{j-1}{m} \rp \rp. 
	\end{align}
	This function is supported on the interval $I_j$, and can be verified to be Lipschitz continuous with parameter $3 m$. Finally, we define the finite subset $\Theta_m = \{\theta_j: j \in [m]\}$ as 
	\begin{align}
		\theta_j(x)= h \phi_j(x) = h \phi\lp m \lp x - \frac{j-1}{m} \rp \rp, \qtext{with} h > 0, \quad 3 m h \leq C. 
	\end{align}
	\item Each function $\theta_j$ in $\Theta_m$  is a scaled-and-shifted bump function that lies in $\Theta$, and also satisfies the separation condition 
	\begin{align}
		\rho(\theta_i, \theta_j) = \|\theta_i - \theta_j\|_\infty = h, \qtext{for all} i \neq j \in [m]. 
	\end{align}
	This simply follows from the fact that $\theta_i$ and $\theta_j$ have disjoint supports for $i \neq j$. Thus, we have constructed a $2\delta$-separated set with $\delta = h/2$. 
	\item To apply Fano's method, we now need to control the statistical distinguishability via the pairwise relative entropy of the associated distributions. Observe that for any $\theta_j, \theta_k$, the distributions $P_j, P_k$ have the form 
	\begin{align}
		P_{\theta_j} = \otimes_{i=1}^n \lp \delta_{x_i} \times N(\theta_j(x_i), \sigma^2) \rp, \qtext{and} 
		P_{\theta_k} = \otimes_{i=1}^n \lp \delta_{x_i} \times N(\theta_k(x_i), \sigma^2) \rp. 
	\end{align}
	\sloppy Thus, the relative entropy between them, which we denote by $D_{jk}$ is completely governed by the $\ell_2$ norm between the ``mean-vectors'' $\mu_j = [\theta_j(1/n), \ldots, \theta_j(n/n) ]$ and $\mu_k= [\theta_k(1/n), \ldots, \theta_k(n/n) ]$; that is, 
	\begin{align}
	D_{jk}= \dkl(P_{\theta_j} \parallel P_{\theta_k}) = \sum_{i=1}^n \frac{(\theta_j(x_i) - \theta_k(x_i))^2}{2 \sigma^2} = \frac{1}{2\sigma^2} \|\mu_j - \mu_k\|_2^2. 
	\end{align}
	Since $\theta_j$ and $\theta_k$ have disjoint supports and take the maximum value of $h$, we have 
	\begin{align}
		D_{jk} \leq \frac{1}{2\sigma^2} h^2 \times 2 \times \lp \frac{n}{m}  + 1\rp \leq \frac{4 h^2 n}{2 \sigma^2 m}. 
	\end{align}
	Plugging this into the maximum relative entropy form of Fano's lower bound, we get 
	\begin{align}
		R_n^* \geq \frac{h}{2} \lp 1 - \frac{2h^2n/\sigma^2m + \log 2}{\log m} \rp. 
	\end{align}
	\item To complete the proof, we need to choose an appropriate value of $m$ and $h$. For a fixed $m$, suppose we choose $h$ to ensure that 
	\begin{align}
		\frac{2 h^2 n}{\sigma^2 m} = \frac{1}{2} \log m \quad \implies \quad 
		h = \frac{\sigma}{2}\sqrt{\frac{m \log m}{n}}. 
	\end{align}
	Assuming $m$ is large enough to ensure that $\log m > 4 \log 2$~(i.e., $m \geq 16$), then with this $h$, the lower bound becomes 
	\begin{align}
		R_n^* \geq \frac{\sigma}{4}\sqrt{\frac{m \log m}{n}} \times \lp 1 - \frac{1}{2} - \frac{1}{4} \rp = 
	\frac{\sigma}{16}\sqrt{\frac{m \log m}{n}}. 
	\end{align}
	\item It remains to make a choice of the parameter $m$. We want to choose $m$ to be as large as possible while ensuring that the constraint on the Lipschitz constants of $\theta_j$ for $j \in [m]$ are satisfied; that is, 
	\begin{align}
		3 m h = 3 m \frac{\sigma}{2} \sqrt{\frac{m \log m}{n}} \leq C \quad \implies \quad 
		m^3 \leq \frac{4C^2 n}{\log m \sigma^2}
	\end{align}
	Without taking the constants into account, a suitable value of $m$ turns out to be 
	\begin{align}
	m^* \asymp \lp \frac{n}{\log n} \rp^{1/3}, \qtext{which implies} R_n^* \gtrsim \lp \frac{\log n}{n} \rp^{1/3}. 
	\end{align}
\end{itemize}

\begin{remark}
Observe that the loss function that we considered does not have the additive structure required to ensure the Hamming separability condition. As a result Assoaud's method would yield a suboptimal lower bound in this problem, as it would not be able to capture the logarithmic factor. 
\end{remark}

\subsection{Achievability}
\label{subsec:nonparametric-regression-achievability}
A simple piecewise constant estimator can achieve the optimal rate for this problem. The idea is to use the same partition we used in the converse derivation and define the estimator $\thetahat$ as 
\begin{align}
\thetahat(x) = \frac{1}{n_j} \sum_{i:x_i \in I_j} Y_i, \; \text{ if } x \in I_j, \qtext{where} n_j = |\{i: x_i \in I_j\}|. 
\end{align}
The error at any point $x \in I_j$ of this estimator satisfies 
\begin{align}
|\thetahat(x) - \theta(x)| = \left\lvert \frac{1}{n_j} \sum_{i: x_i \in I_j}\lp \theta(x_i) + \sigma \varepsilon_i \rp - \theta(x) \right\rvert \leq \sigma \bar{\varepsilon}_j + \left\lvert \bar{\mu}_j - \theta(x) \right\rvert, 
\end{align}
where we use the notation 
\begin{align}
\bar{\varepsilon}_j = \frac{1}{n_j} \sum_{i: x_i \in I_j} \varepsilon_j, \sim N(0, \frac{1}{n_j}), \qtext{and} 
\bar{\mu}_j =  \frac{1}{n_j}\sum_{i: x_i \in I_j}  \theta(x_i). 
\end{align}
Hence, the estimation risk is upper bound by 
\begin{align}
\mathbb{E}_{\theta}\lb \|\thetahat(X^n) - \theta\|_\infty \rb \leq  \sigma \mathbb{E} \lb \max_{j \in [m]} \bar{\varepsilon}_j \rb + \max_{j} \sup_{x \in I_j} |\bar{\mu}_j - \theta(x)|. \label{eq:achievability-1}
\end{align}
Now, a standard fact about Gaussian random variables $Z_1, \ldots, Z_m \simiid N(0, a^2)$ is that 
\begin{align}
	\mathbb{E}[\max_{j} |Z_j|] \leq a \sqrt{2 \log 2m}, \quad \implies \quad  
	\mathbb{E}[\max_j |\bar{\varepsilon}_j|] \leq \sqrt{\frac{2 \log 2m}{n_j}} \asymp \sqrt{\frac{2m \log 2m}{n}}. 
\end{align}
The second term in~\eqref{eq:achievability-1} is controlled by the Lipschitz property of $\theta$: 
\begin{align}
	\max_{j} \sup_{x \in I_j} |\bar{\mu}_j - \theta(x)| \leq C \frac{1}{m}. 
\end{align}
Combining these two facts, we get the following upper bound on the minimax risk of $\thetahat$, 
\begin{align}
R_n(\thetahat, \Theta) = \sup_{\theta \in \Theta} R_n(\thetahat, \theta) \leq \frac{C}{m} + \sqrt{\frac{2m \log 2m}{n}}. 
\end{align}
Choosing $m \asymp (\log n / n)^{1/3}$ then gives us the required minimax rate 
\begin{align}
R_n(\thetahat, \Theta) \lesssim \lp \frac{\log n}{n} \rp^{1/3}. 
\end{align}
\begin{remark}
	\label{remark:sub-Gaussian} Observe the we used the fact that the noise is Gaussian only while obtaining an upper bound on $\mathbb{E}[\max_j |\bar{\varepsilon}_j|]$, which relies on the fact that the Gaussian random variables have a light tail. Hence, the same argument carries through to the case of a larger family of light tailed noise distributions called sub-Gaussian distributions. Furthermore, for this family, our results can be easily extended to hold with high probability (rather than in expectation). The corresponding high probability lower bound derivation however, will require more powerful versions of Fano's inequality. 
\end{remark}
\begin{remark}
	\label{remark:higher-order-smoothness} While our focus in this lecture was on considering the simple case of $\gamma \leq 1$, we note that very similar ideas can be applied for functions with higher order smoothness $\gamma >1$. The only difference is that we have to employ more powerful estimators such as local polynomial~(instead of constant) estimators, kernel smoothing etc. 
\end{remark}
% \newpage
% \bibliographystyle{abbrvnat}           % if you need a bibliography
% \bibliography{../ref}                % assuming yours is named ref.bib


\end{document}